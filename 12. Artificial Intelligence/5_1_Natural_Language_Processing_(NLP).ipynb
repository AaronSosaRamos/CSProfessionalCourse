{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "zK5tOqE_IOLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) is a field of artificial intelligence focused on the interaction between computers and humans' natural language. It enables computers to understand, interpret, and generate human language in a way that is both useful and meaningful.\n",
        "\n"
      ],
      "metadata": {
        "id": "BxGpS6AyIPLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corpus-based Methods"
      ],
      "metadata": {
        "id": "TjLhRdgYIbf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corpus-based methods in NLP involve analyzing large collections of texts (corpora) to extract patterns, relationships, and insights. These methods often include techniques such as statistical analysis, machine learning, and linguistic processing to understand and manipulate natural language data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ePXJ44NqIdwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW0x4FPfH16V",
        "outputId": "75e935ee-31fb-40dc-fd68-c9bb1ac75e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'The': 3, 'cat': 1, 'sat': 1, 'on': 1, 'the': 1, 'mat.': 1, 'dog': 1, 'barked': 1, 'loudly.': 1, 'sun': 1, 'is': 1, 'shining.': 1})\n"
          ]
        }
      ],
      "source": [
        "# Example of corpus-based method: Counting word frequencies in a corpus\n",
        "from collections import Counter\n",
        "\n",
        "corpus = [\"The cat sat on the mat.\", \"The dog barked loudly.\", \"The sun is shining.\"]\n",
        "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "word_counts = Counter(word for sentence in tokenized_corpus for word in sentence)\n",
        "\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#N-gram Representation"
      ],
      "metadata": {
        "id": "nXFrgz3NIjDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-grams are contiguous sequences of n items from a given sample of text or speech. In NLP, representing text using N-grams can capture local word dependencies and provide useful features for tasks such as language modeling, text generation, and sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "p-OImeAUImze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of representing text using N-grams\n",
        "from nltk import ngrams\n",
        "\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "n = 2  # Bi-grams\n",
        "tokenized_sentence = sentence.split()\n",
        "bi_grams = list(ngrams(tokenized_sentence, n))\n",
        "\n",
        "print(bi_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv3Wxf2yIlVn",
        "outputId": "90a48fa3-2bd2-44ee-f21d-c5ad97223fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords and Lemmatization"
      ],
      "metadata": {
        "id": "9CgB3gI6JHJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are common words (e.g., \"and\", \"the\", \"is\") that are often filtered out from text data because they do not carry significant meaning. Lemmatization is the process of reducing words to their base or dictionary form (lemmas) to normalize variations of words."
      ],
      "metadata": {
        "id": "mXAIdBz3JHhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of stop-word removal and lemmatization using NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"The cats are sitting on the mats and dogs are barking.\"\n",
        "tokenized_text = word_tokenize(text)\n",
        "filtered_text = [word for word in tokenized_text if word.lower() not in stop_words]\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
        "\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AayBGgbLJJgX",
        "outputId": "4fec7680-2e89-4b0a-fbb4-12fa537bda51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'sitting', 'mat', 'dog', 'barking', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Classification"
      ],
      "metadata": {
        "id": "vmH2kjjPJp8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text classification is the task of assigning predefined categories or labels to textual documents based on their content. It is a fundamental problem in NLP with applications in sentiment analysis, spam detection, topic categorization, and more."
      ],
      "metadata": {
        "id": "_fiMq_tEJql2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of text classification using scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Training data\n",
        "X_train = [\"I love this movie.\", \"This movie is awful.\", \"The plot is interesting.\", \"This movie is terrible\"]\n",
        "y_train = [\"positive\", \"negative\", \"positive\", \"negative\"]\n",
        "\n",
        "# Test data\n",
        "X_test = [\"I enjoyed the film.\", \"The movie was terrible.\"]\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB classifier\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHJYUPf0JuTQ",
        "outputId": "6082ea4c-4371-4046-b4a5-bf41fe1a292b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive' 'negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Some NLP models:"
      ],
      "metadata": {
        "id": "DuZC3MrMJ6EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity Recognition (NER) Model"
      ],
      "metadata": {
        "id": "c_QghI2AJ45H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a task in NLP that involves identifying and classifying named entities (such as names of persons, organizations, locations, etc.) in text data. It is often used for information extraction and text understanding tasks."
      ],
      "metadata": {
        "id": "XvzbZMj0J5Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Named Entity Recognition using spaCy\n",
        "import spacy\n",
        "\n",
        "# Load English NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Text to analyze\n",
        "text = \"Apple is headquartered in Cupertino, California.\"\n",
        "\n",
        "# Analyze text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOwrB3fAJ99f",
        "outputId": "256192a8-4335-46bd-d153-3ca8fc25630e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Cupertino GPE\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embedding Model"
      ],
      "metadata": {
        "id": "HwI11ZgBKJ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings are dense vector representations of words in a high-dimensional space where the similarity between words is captured by the proximity of their vectors. They are widely used in NLP tasks such as semantic similarity, language translation, and document clustering."
      ],
      "metadata": {
        "id": "CSXmgpp_KVbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Word Embedding using Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example sentences\n",
        "sentences = [[\"cat\", \"sat\", \"mat\"], [\"dog\", \"barked\", \"loudly\"], [\"sun\", \"shining\"]]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get word vector\n",
        "vector = model.wv['cat']\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvCvyiDuKMe-",
        "outputId": "6a60dbd5-6bcb-461d-8086-2bf034d8ad80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Summarization Model"
      ],
      "metadata": {
        "id": "ccHI8M8tKX41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text summarization is the process of distilling the most important information from a text document to produce a concise summary. It can be done through extractive methods (selecting important sentences) or abstractive methods (generating new sentences)."
      ],
      "metadata": {
        "id": "xmR-LtifKY8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Filter out stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_freq = FreqDist(filtered_words)\n",
        "\n",
        "    # Calculate sentence scores based on word frequencies\n",
        "    sentence_scores = {sentence: sum(word_freq[word] for word in word_tokenize(sentence.lower()) if word in word_freq)\n",
        "                       for sentence in sentences}\n",
        "\n",
        "    # Sort sentences by scores and select the top N sentences\n",
        "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "\n",
        "    # Return the summarized text\n",
        "    summarized_text = ' '.join(top_sentences)\n",
        "    return summarized_text\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a field of artificial intelligence focused on the interaction between computers and humans' natural language. It enables computers to understand, interpret, and generate human language in a way that is both useful and meaningful. Text summarization is the process of distilling the most important information from a text document to produce a concise summary. It can be done through extractive methods (selecting important sentences) or abstractive methods (generating new sentences).\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the text\n",
        "summary = summarize_text(text)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSaXXiq_Kc5F",
        "outputId": "19e11aa4-6cd3-47d5-b1da-9939591b77a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It can be done through extractive methods (selecting important sentences) or abstractive methods (generating new sentences). \n",
            "Natural Language Processing (NLP) is a field of artificial intelligence focused on the interaction between computers and humans' natural language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "TkKiMJ4HKtD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis aims to determine the sentiment or opinion expressed in a piece of text. It can be binary (positive or negative) or multiclass (positive, neutral, negative), and it is useful for understanding customer feedback, social media sentiment, and market trends.\n",
        "\n"
      ],
      "metadata": {
        "id": "QdyEF0SKKtae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Sentiment Analysis using VADER\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Text to analyze\n",
        "text = \"I love this movie. It's fantastic!\"\n",
        "\n",
        "# Sentiment analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "# Print sentiment scores\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R8qa4rjKpy-",
        "outputId": "5e16b9d1-ced1-4f6b-b03b-341af21a774f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.27, 'pos': 0.73, 'compound': 0.8439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation Model"
      ],
      "metadata": {
        "id": "xeoS1XLAK2u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation involves generating new text based on a given input or prompt. It can be done using various techniques such as Markov chains, recurrent neural networks (RNNs), or transformers. Here's an example using a simple Markov chain approach:\n",
        "\n"
      ],
      "metadata": {
        "id": "t96OyV6yK3JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install markovify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWCKLVxCK8bu",
        "outputId": "cec44e5f-9a45-41a5-c693-132449b59fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18608 sha256=d5d86414383a08808abf5554f3802a59a029bb7d30d80ae18145394d9f70d908\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/8c/c5/41413e24c484f883a100c63ca7b3b0362b7c6f6eb6d7c9cc7f\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markovify\n",
        "\n",
        "# Larger corpus of text for training\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog barked loudly.\",\n",
        "    \"The sun is shining.\",\n",
        "    \"She sells seashells by the seashore.\",\n",
        "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
        "]\n",
        "\n",
        "# Build a Markov chain model\n",
        "text_model = markovify.Text(corpus)\n",
        "\n",
        "# Generate text\n",
        "generated_text = text_model.make_sentence()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvXjV6FbK4P3",
        "outputId": "9f0eab9b-0b74-4ba0-d60d-1ac5b77f4582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recurrent Neural Network:"
      ],
      "metadata": {
        "id": "01M-syf-LfDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Positive: Indicates that the review expresses a positive sentiment towards the movie.\n",
        "* Negative: Indicates that the review expresses a negative sentiment towards the movie."
      ],
      "metadata": {
        "id": "OzLdxGjJLmSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the IMDb dataset\n",
        "num_words = 10000\n",
        "max_len = 200\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RNN architecture\n",
        "embedding_dim = 128\n",
        "units = 64\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(SpatialDropout1D(dropout_rate))\n",
        "model.add(LSTM(units=units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
        "model.add(LSTM(units=units, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss = BinaryCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# Example predictions\n",
        "predictions = model.predict(X_test[:10])\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khyW1DP5LLaG",
        "outputId": "177f5003-6262-42e9-efe3-5f7f4abf105a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 212s 1s/step - loss: 0.4294 - accuracy: 0.7915 - val_loss: 0.3327 - val_accuracy: 0.8618\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 204s 1s/step - loss: 0.2467 - accuracy: 0.9054 - val_loss: 0.2883 - val_accuracy: 0.8772\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 205s 1s/step - loss: 0.1971 - accuracy: 0.9258 - val_loss: 0.3171 - val_accuracy: 0.8682\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 206s 1s/step - loss: 0.1436 - accuracy: 0.9481 - val_loss: 0.4193 - val_accuracy: 0.8658\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 204s 1s/step - loss: 0.1022 - accuracy: 0.9645 - val_loss: 0.4052 - val_accuracy: 0.8676\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.4319 - accuracy: 0.8571\n",
            "Test Loss: 0.4318576157093048, Test Accuracy: 0.8570799827575684\n",
            "1/1 [==============================] - 1s 550ms/step\n",
            "[[0.2337942 ]\n",
            " [0.9995297 ]\n",
            " [0.99739015]\n",
            " [0.9942407 ]\n",
            " [0.9995601 ]\n",
            " [0.9973126 ]\n",
            " [0.9983939 ]\n",
            " [0.00730167]\n",
            " [0.99783653]\n",
            " [0.9969639 ]]\n"
          ]
        }
      ]
    }
  ]
}